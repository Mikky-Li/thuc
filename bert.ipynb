{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3d14812-a703-4b8d-8530-e73831da9cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d550ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2efd3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bertconfig():\n",
    "    def __init__(self):\n",
    "        self.project = 'THUC'\n",
    "        self.bert_path = './bert_pretrain'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "        self.class_list = [x.strip() for x in open('./THUC/data/class.txt').readlines()]\n",
    "        self.pad_size = 32\n",
    "        self.batch_size = 128\n",
    "        self.device = 'cuda'\n",
    "        self.hidden_size = 768\n",
    "        self.num_classes = len(self.class_list)\n",
    "        self.learning_rate = 1e-5\n",
    "        self.num_epochs = 10\n",
    "        self.save_path = './THUC/'+ self.project+'.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79910406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = bertconfig()\n",
    "config.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb4e6c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(pad_size):\n",
    "    def load_data(path,pad_size):\n",
    "        input = []\n",
    "        label = []\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                content = line.strip().split('\\t')[0]\n",
    "                label = line.strip().split('\\t')[1]\n",
    "                token = config.tokenizer.tokenize(content)\n",
    "                token = ['CLS']+token\n",
    "                token_ids = config.tokenizer.convert_tokens_to_ids(token)\n",
    "                seq_len = len(token_ids)\n",
    "                if len(token_ids)<pad_size:\n",
    "                    token_ids = token_ids+[0]*(pad_size-len(token_ids))\n",
    "                    mask = [1]*seq_len+[0]*(pad_size-seq_len)\n",
    "                else:\n",
    "                    token_ids = token_ids[:pad_size]\n",
    "                    seq_len = pad_size\n",
    "                    mask = [1]*seq_len\n",
    "                input.append((token_ids,int(label),seq_len,mask))\n",
    "        return input\n",
    "    train_data = load_data('./THUC/data/train.txt',pad_size)\n",
    "    test_data = load_data('./THUC/data/test.txt',pad_size)\n",
    "    dev_data = load_data('./THUC/data/dev.txt',pad_size)\n",
    "    return train_data,test_data,dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e815b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33590it [00:07, 4333.41it/s]\n",
      "10000it [00:02, 4005.17it/s]\n",
      "10000it [00:02, 4188.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data,dev_data = build_dataset(config.pad_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b283e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterator(object):\n",
    "    def __init__(self, batches, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = batches\n",
    "        self.n_batches = len(batches) // batch_size\n",
    "        self.residue = False \n",
    "        if len(batches) % self.n_batches != 0:\n",
    "            self.residue = True\n",
    "        self.index = 0\n",
    "        self.device = device\n",
    "        \n",
    "    def _to_tensor(self, datas):\n",
    "        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n",
    "        return (x, seq_len, mask), y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.residue and self.index == self.n_batches:\n",
    "            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.residue:\n",
    "            return self.n_batches + 1\n",
    "        else:\n",
    "            return self.n_batches\n",
    "\n",
    "\n",
    "def build_iterator(dataset, config):\n",
    "    iter = DatasetIterator(dataset, config.batch_size, config.device)\n",
    "    return iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d13f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = build_iterator(train_data, config)\n",
    "test_iter = build_iterator(test_data, config)\n",
    "dev_iter = build_iterator(dev_data, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19485dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): \n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子\n",
    "        mask = x[2]  # mask\n",
    "        _ = self.bert(context, attention_mask=mask)\n",
    "        out = self.fc(_[1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b894ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_pretrain were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model(config).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0aceebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a16237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "Batch:0\n",
      "Val Loss:0.7173303365707397\n",
      "Val Accuracy:0.0989\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [01:49,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "Batch:50\n",
      "Val Loss:0.37868258357048035\n",
      "Val Accuracy:0.7672\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [03:40,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "Batch:100\n",
      "Val Loss:0.16982828080654144\n",
      "Val Accuracy:0.852\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [05:30,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "Batch:150\n",
      "Val Loss:0.1311345398426056\n",
      "Val Accuracy:0.8785\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [07:21,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "Batch:200\n",
      "Val Loss:0.11852074414491653\n",
      "Val Accuracy:0.8848\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [09:11,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "Batch:250\n",
      "Val Loss:0.10721858590841293\n",
      "Val Accuracy:0.8972\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [10:07,  2.31s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2\n",
      "Batch:0\n",
      "Val Loss:0.10714345425367355\n",
      "Val Accuracy:0.8967\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [01:49,  1.44s/it]"
     ]
    }
   ],
   "source": [
    "best_dev_loss = np.inf\n",
    "for epoch in range(config.num_epochs):\n",
    "    idx = 0\n",
    "    for i, (trains, labels) in tqdm(enumerate(train_iter)):\n",
    "        model.train()\n",
    "        outputs = model(trains)\n",
    "        model.zero_grad()\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #每50个batch update一次\n",
    "        if idx%50==0:\n",
    "            true = labels.data.cpu()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "            train_acc = metrics.accuracy_score(true, predic)\n",
    "            model.eval()\n",
    "            loss_total = 0\n",
    "            predict_all = np.array([], dtype=int)\n",
    "            labels_all = np.array([], dtype=int)\n",
    "            with torch.no_grad():\n",
    "                for texts, labels in dev_iter:\n",
    "                    outputs = model(texts)\n",
    "                    loss = F.cross_entropy(outputs, labels)\n",
    "                    loss_total += loss\n",
    "                    labels = labels.data.cpu().numpy()\n",
    "                    predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "                    labels_all = np.append(labels_all, labels)\n",
    "                    predict_all = np.append(predict_all, predic)\n",
    "            acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "            print('Epoch:{0}'.format(epoch+1))\n",
    "            print('Batch:{0}'.format(idx))\n",
    "            print('Val Loss:{0}'.format(loss_total/len(train_iter)))\n",
    "            print('Val Accuracy:{0}'.format(acc))\n",
    "            if loss_total/len(train_iter) < best_dev_loss:\n",
    "                print('Model Saved!')\n",
    "                best_dev_loss = loss_total/len(train_iter)\n",
    "                torch.save(model.state_dict(), config.save_path) \n",
    "        idx+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
