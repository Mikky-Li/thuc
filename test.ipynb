{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb54edc5-1894-497d-b4ea-42fdc0ba1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 18.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 41.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 29.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.7.0 tokenizers-0.12.1 transformers-4.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f80153-62b2-4c6f-8bc9-da6bb22c4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939520d4-ae3b-43c6-9f88-276fdf13eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bertconfig():\n",
    "    def __init__(self):\n",
    "        self.project = 'THUC'\n",
    "        self.bert_path = './bert_pretrain'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "        self.class_list = [x.strip() for x in open('./THUC/data/class.txt').readlines()]\n",
    "        self.pad_size = 32\n",
    "        self.batch_size = 128\n",
    "        self.device = 'cuda'\n",
    "        self.hidden_size = 768\n",
    "        self.num_classes = len(self.class_list)\n",
    "        self.learning_rate = 1e-5\n",
    "        self.num_epochs = 10\n",
    "        self.save_path = './THUC/'+ self.project+'.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a440cc-882d-4e8f-b285-9ad28326016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = bertconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c7938e6-bc1f-4cf5-a6e6-90cf3d9e01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): \n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path)\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        context = x[0].unsqueeze(0)  # 输入的句子\n",
    "        mask = x[2].unsqueeze(0)  # mask\n",
    "        _ = self.bert(context, attention_mask=mask)\n",
    "        out = self.fc(_[1])\n",
    "        prob = F.softmax(out)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31e060cf-e60b-4861-be2d-b9cb58f7bd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_pretrain were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model(config).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcc9eb93-e50e-40e0-9dfd-4dfde93d55b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(config.save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b671f9b-2e67-41f7-a17f-1c2056edadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sentence,config):\n",
    "    token = config.tokenizer.tokenize(sentence)\n",
    "    token = ['CLS']+token\n",
    "    token_ids = config.tokenizer.convert_tokens_to_ids(token)\n",
    "    seq_len = len(token_ids)\n",
    "    pad_size = config.pad_size\n",
    "    if len(token_ids)<pad_size:\n",
    "        token_ids = token_ids+[0]*(pad_size-len(token_ids))\n",
    "        mask = [1]*seq_len+[0]*(pad_size-seq_len)\n",
    "    else:\n",
    "        token_ids = token_ids[:pad_size]\n",
    "        seq_len = pad_size\n",
    "        mask = [1]*seq_len\n",
    "    input = (torch.LongTensor(token_ids).to(config.device),\n",
    "             torch.LongTensor(seq_len).to(config.device),\n",
    "             torch.LongTensor(mask).to(config.device))\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7efd830-88d8-49a1-8739-ad4cd3b962ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = load_data('中超-10人广州1-0击败10人河北 武汉三镇2-1上海海港',config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dce082de-ca30-4b6c-ac33-f38f089493f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112/337860286.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(out)\n"
     ]
    }
   ],
   "source": [
    "label = config.class_list[torch.argmax(model(input))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec246fd5-a7e3-4c36-8c01-5da700f8be4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sports'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
